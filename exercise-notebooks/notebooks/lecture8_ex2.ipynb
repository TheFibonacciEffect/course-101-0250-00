{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 2 - **Multi-XPU computing**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The goal of this exercise is to:\n",
    "- Familiarise with distributed computing\n",
    "- Combine [ImplicitGlobalGrid.jl](https://github.com/eth-cscs/ImplicitGlobalGrid.jl) and [ParallelStencil.jl](https://github.com/omlins/ParallelStencil.jl)\n",
    "- Learn about GPU MPI on the way"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this exercise, you will:\n",
    "- Create a 2D multi-XPU acoustic wave solver\n",
    "- Make it XPU compatible using ParallelStencil.jl\n",
    "- Deploy it on multiple XPUs using ImplicitGlobalGrid.jl\n",
    "\n",
    "### Task 1\n",
    "\n",
    "Starting from the `acoustic_2D_perf_xpu.jl` script you developed for exercise 2 (lecture 7) (also available in the [scripts/](https://github.com/eth-vaw-glaciology/course-101-0250-00/blob/main/scripts/) folder after homework 7 due date), create a multi-XPU version adding ImplicitGlobalGrid features to run on multiple XPUs.\n",
    "\n",
    "Follow the steps discussed in lecture 8 in the [Using `ImplicitGlobalGrid.jl`](#using_implicitglobalgridjl) Section.\n",
    "\n",
    "Create a new `acoustic_2D_perf_mulixpu.jl` script with following physics and numerics parameters:\n",
    "\n",
    "```julia\n",
    "# Physics\n",
    "Lx, Ly  = 10.0, 10.0\n",
    "œÅ       = 1.0\n",
    "K       = 1.0\n",
    "ttot    = 1e1\n",
    "# Numerics\n",
    "nx, ny  = 32, 32 # number of grid points (local domain)\n",
    "nout    = 10\n",
    "```\n",
    "\n",
    "Add the \"hide communication\" feature using a a boundary width of `(8,2)` as such `@hide_communication (8, 2) begin ...`\n",
    "\n",
    "Add the same visualisation routine as for the multi-XPU diffusion example from lecture 8, automatically creating a .gif (in this case for pressure `P` instead of concentration `C`). You can use the following plotting options\n",
    "```julia\n",
    "opts = (aspect_ratio=1, xlims=(Xi_g[1], Xi_g[end]), ylims=(Yi_g[1], Yi_g[end]), clims=(-0.25, 0.25), c=:davos, xlabel=\"Lx\", ylabel=\"Ly\", title=\"time = $(round(it*dt, sigdigits=3))\")\n",
    "heatmap(Xi_g, Yi_g, Array(P_v)'; opts...); frame(anim)\n",
    "```\n",
    "\n",
    "### Task 2\n",
    "\n",
    "Execute the `acoustic_2D_perf_mulixpu.jl` on 4 GPUs (on your octopus node) to generate a .gif. Then, create a new section in the `README.md` of your lecture 8 GitHub repository and add the generated .gif. Including a short description of the results.\n",
    "\n",
    "### Task 3\n",
    "\n",
    "Realise a weak scaling experiment on 1-4 Titan Xm GPUs on your node and report parallel efficiency.\n",
    "\n",
    "Select the number of grid points for the local domain (`nx`, `ny`) to match the best $T_\\mathrm{eff}$ values reported in [Lecture 7 - Task 3](/lecture7/#task_3__2). Make sure to deactivate visualisation and adapt the total time `ttot` or the number of time steps `nt` such that the code executes in about one second.\n",
    "\n",
    "Report the execution time `t1 = t_toc` for the single GPU execution. Then, run the code on 1, 2, 3 and 4 GPUs saving the `t_toc` time for each run. Report the parallel efficiency by normalising the execution time `t_toc` for the 1-4 GPU runs by `t1`. Plot the results as function of number of GPUs.\n",
    "\n",
    "Add the figure to a new section of the `README.md` and briefly discuss the results. Make sure to report to local problem size you used when performing the weak scaling test."
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "kernelspec": {
   "name": "julia-1.6",
   "display_name": "Julia 1.6.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
